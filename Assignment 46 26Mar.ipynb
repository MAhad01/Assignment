{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556c8f35-de94-47e8-94ef-b9fce0ec54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# Simple linear regression is used when there is only one independent variable that can be used to predict the dependent variable. \n",
    "# For example, if we want to predict the price of a house based on its size alone, we can use simple linear regression. \n",
    "# Multiple linear regression is used when there are two or more independent variables that can be used to predict the dependent variable.\n",
    "# For example, if we want to predict the price of a house based on its size and location, we can use multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624d24bd-6137-42c8-95c9-a692e62d599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# There are four assumptions of linear regression that must be met before we can use linear regression to analyze our data. These assumptions are:\n",
    "\n",
    "# Linear relationship: There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n",
    "# Independence: The residuals are independent.\n",
    "# Homoscedasticity: The residuals have constant variance at every level of x.\n",
    "# Normality: The residuals of the model are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cbbc2f2-6397-41ad-b786-3c93255a7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# The slope of a linear regression model represents the average change in the dependent variable for a one-unit increase in the independent variable. \n",
    "# The intercept represents the value of the dependent variable when the independent variable is equal to zero.\n",
    "# For example, if we have a simple linear regression model that predicts weight based on height,\n",
    "# the slope would represent the average increase in weight for a one-inch increase in height, and the intercept would represent the \n",
    "# average weight of someone who is zero inches tall (which is not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5098ed9-987a-4a9f-a862-7a7d90404cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# Gradient descent is an optimization algorithm used to minimize the cost function in machine learning. The cost function is a measure of how well the model fits the data. \n",
    "# Gradient descent works by iteratively adjusting the model parameters in the direction of steepest descent of the cost function until it reaches a minimum .\n",
    "\n",
    "# The basic idea behind gradient descent is to calculate the gradient of the cost function with respect to each parameter in the model. \n",
    "# The gradient is a vector that points in the direction of steepest ascent of the cost function. \n",
    "# By taking steps in the opposite direction of the gradient, we can iteratively adjust the model parameters until we reach a minimum.\n",
    "\n",
    "# There are two main types of gradient descent: batch gradient descent and stochastic gradient descent.\n",
    "# Batch gradient descent calculates the gradient over the entire training set, while stochastic gradient descent calculates the gradient over a single training example at a time. \n",
    "# Stochastic gradient descent is faster but less accurate than batch gradient descent ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d8b772-80ff-4419-a2f8-7386c28a6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# Multiple linear regression is a statistical method used to model the relationship between two or more independent variables and a dependent variable. \n",
    "# It is an extension of simple linear regression, which models the relationship between one independent variable and a dependent variable.\n",
    "\n",
    "# In multiple linear regression, the model takes the form:\n",
    "\n",
    "# y = b0 + b1x1 + b2x2 + … + bnxn + e\n",
    "\n",
    "# where y is the dependent variable, x1, x2, …, xn are the independent variables, b0 is the intercept, and b1, b2, …, bn are the coefficients for each independent variable. \n",
    "# The coefficients represent the average change in y for a one-unit increase in each independent variable, holding all other variables constant.\n",
    "\n",
    "# The main difference between simple linear regression and multiple linear regression is that simple linear regression models the relationship between one independent variable \n",
    "# and a dependent variable, while multiple linear regression models the relationship between two or more independent variables and a dependent variable ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001c4913-3e45-4e56-b50b-2cb41aca3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Multicollinearity is a phenomenon that occurs when two or more independent variables in a multiple linear regression model are highly correlated with each other. \n",
    "# This can cause problems in the model because it makes it difficult to determine the effect of each independent variable on the dependent variable.\n",
    "\n",
    "# To detect multicollinearity, you can calculate the correlation matrix between all pairs of independent variables. \n",
    "# If there is a high correlation between two or more independent variables (i.e., a correlation coefficient greater than 0.7), then there may be multicollinearity in the model.\n",
    "\n",
    "# To address multicollinearity, you can either remove one or more of the highly correlated independent variables from the model or combine them into a single variable. \n",
    "# Another approach is to use regularization techniques such as ridge regression or lasso regression, which can help to reduce the impact of multicollinearity on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2d4cd5-c389-4945-b0d6-193a3c079571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. \n",
    "# This allows us to model nonlinear relationships between the variables.\n",
    "\n",
    "# The polynomial regression model takes the form:\n",
    "\n",
    "# y = b0 + b1x + b2x^2 + … + bnx^n + e\n",
    "\n",
    "# where y is the dependent variable, x is the independent variable, b0, b1, …, bn are the coefficients for each term in the polynomial, and e is the error term.\n",
    "\n",
    "# The main difference between linear regression and polynomial regression is that linear regression models the relationship between one independent variable \n",
    "# and a dependent variable using a straight line, while polynomial regression models the relationship between one independent variable and a dependent variable using a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53ccb48-c5d0-49d7-90f2-488444b70098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# he advantages of polynomial regression over linear regression are that it can model nonlinear relationships between the variables and can fit the data more accurately. \n",
    "# The disadvantages are that it can be more complex and difficult to interpret than linear regression and can be prone to overfitting.\n",
    "\n",
    "# Polynomial regression is useful when the relationship between the independent variable and the dependent variable is nonlinear. \n",
    "# For example, if we are modeling the relationship between temperature and ice cream sales, we might use polynomial regression because the relationship is likely to be nonlinear.\n",
    "\n",
    "# Linear regression is useful when the relationship between the independent variable and the dependent variable is linear. \n",
    "# For example, if we are modeling the relationship between height and weight, we might use linear regression because the relationship is likely to be linear."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
