{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926e454c-d487-4567-a8c3-95f1d7af7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "#  A projection in PCA is a transformation of data from a higher-dimensional space to a lower-dimensional space.\n",
    "#     The goal is to find a lower-dimensional space where the variance of the projected data is maximized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd3c31c-f1c2-40ab-9018-8c9516993297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# he optimization problem in PCA aims to find the directions (principal components) in the data that capture the maximum variance.\n",
    "# It does this by solving an eigenvalue problem for the covariance matrix of the data. \n",
    "# The principal components are the eigenvectors of this covariance matrix, and the amount of variance captured by each principal component is given by the corresponding eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1c0e1a-fdc4-430e-8948-4d76bf0fb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "#  The covariance matrix is a key part of PCA. It quantifies the degree to which different dimensions in the data vary together. \n",
    "#     PCA uses the covariance matrix to find the directions (principal components) that capture the most variance in the data. \n",
    "#     These directions are the eigenvectors of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29ef6a5-1164-4a13-ab2a-2a010e7e9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "#  The choice of the number of principal components in PCA impacts the balance between dimensionality reduction and information retention. \n",
    "#     Using more principal components will retain more information (variance) but will result in a higher-dimensional representation. \n",
    "#     Conversely, using fewer principal components will result in a lower-dimensional representation but may discard some information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61ffb427-9e55-48d2-adc5-ea1c281afdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# PCA can be used in feature selection by transforming the original features into a new set of uncorrelated features (principal components).\n",
    "# These principal components capture the most variance in the data, making them potentially more informative than the original features.\n",
    "# This can help to improve the performance of machine learning models by reducing the risk of overfitting and reducing computational complexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a42160b-4bc6-4520-8577-dabaefabe42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# PCA has many applications in data science and machine learning. \n",
    "# It is commonly used for dimensionality reduction, exploratory data analysis, and data visualization. \n",
    "# It can also be used in various fields ranging from neuroscience to quantitative finance, with applications including image compression,\n",
    "# facial recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a3f5c6a-5594-4108-a851-570d522ec050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "#  In PCA, spread and variance are closely related concepts. \n",
    "# The spread of the data refers to how much the data varies, and variance is a specific measure of this variation. \n",
    "# PCA seeks to find the directions (principal components) that capture the maximum variance, or equivalently, the maximum spread in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c334279-816a-41fa-9060-79ee93f7d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# PCA identifies the principal components by finding the directions in the data that capture the maximum spread (variance). \n",
    "# It does this by solving an eigenvalue problem for the covariance matrix of the data.\n",
    "# The principal components are the eigenvectors of this covariance matrix, and the amount of variance (spread) captured by each principal\n",
    "# component is given by the corresponding eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd71c1-be11-498b-9db7-674c614a9c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9\n",
    "\n",
    "#  PCA handles data with high variance in some dimensions and low variance in others by transforming the data to align with the directions\n",
    "#     of maximum variance. This means that the principal components (the new dimensions) are chosen to align with the directions in the data that have the highest variance. \n",
    "#     Therefore, dimensions in the original data that have low variance will have less influence on the principal components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
