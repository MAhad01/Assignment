{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638f2588-24f2-4102-b329-67d6866c75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# The K-Nearest Neighbor (KNN) algorithm is a non-parametric supervised learning method.\n",
    "# It’s used for both classification and regression. The algorithm works on the principle that similar data points tend to have similar \n",
    "# labels or values.\n",
    "\n",
    "# During the training phase, the KNN algorithm stores the entire training dataset.\n",
    "# When a new data point appears, it classifies that data into a category that is most similar to the available categories.\n",
    "# This is done by calculating the distance between the new data point and each of the training data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81736a43-9aff-44e7-98b7-2045933de98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is crucial and depends on the specific problem and dataset.\n",
    "# Here are some common conventions to keep in mind:\n",
    "\n",
    "# Odd Value: It’s recommended to choose an odd value for K to avoid ties in classification.\n",
    "# Avoid Noise and Outliers: If the input data has more outliers or noise, a higher value of K would be better.\n",
    "# Rule of Thumb: A common rule of thumb is to set K = sqrt(N)/2, where N stands for the number of samples in your training dataset.\n",
    "# Experimentation: There is no straightforward method to calculate K. You have to experiment with different values to choose the optimal K\n",
    "# for your problem and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825fc0d9-0868-493a-85a5-ee7736542e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# The K-Nearest Neighbors (KNN) algorithm can be used for both classification and regression tasks.\n",
    "# The difference between KNN classifier and KNN regressor lies in the way they make predictions:\n",
    "\n",
    "# KNN Classifier: The KNN classifier predicts the class of a new data point by looking at the classes of its K nearest neighbors.\n",
    "# The new data point is assigned to the class that has the highest frequency among its K nearest neighbors.\n",
    "\n",
    "# KNN Regressor: On the other hand, the KNN regressor predicts the value of a new data point by calculating the average of the values of\n",
    "# its K nearest neighbors213. Instead of predicting a class, the regressor uses the average of all the neighbor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad10dea5-5973-43e1-8821-5ce0a227a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# The performance of the K-Nearest Neighbors (KNN) algorithm can be measured using several metrics12:\n",
    "\n",
    "# Accuracy: This is the most intuitive performance measure, and it is defined as the ratio of the number of correctly classified objects to the total number of objects evaluated1.\n",
    "\n",
    "# Precision: It is the ratio of correctly predicted positive data objects to the total predicted positive data objects1.\n",
    "\n",
    "# Recall: It is the ratio of correctly predicted positive observations to all observations in actual class.\n",
    "\n",
    "# F1 Score: It is the weighted average of Precision and Recall. It tries to find the balance between precision and recall.\n",
    "\n",
    "# Area Under ROC (Receiver Operating Characteristic) Curve (AUC-ROC): ROC curve is a plot of true positive rate against false positive rate. The area under this curve is used as a measure of separability of the classes.\n",
    "\n",
    "# Cross-Validation: This is a technique used to assess the performance of machine learning models. It divides the data into subsets and then trains the algorithm on these subsets.\n",
    "\n",
    "# Confusion Matrix: This is a table that describes the performance of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a330a36-d794-4cca-867d-4a9b93aa37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# The “curse of dimensionality” in the context of the K-Nearest Neighbors (KNN) algorithm refers to the challenges \n",
    "# and problems that arise when dealing with high-dimensional data.\n",
    "\n",
    "# In KNN, the algorithm fundamentally relies on a distance metric to find the nearest neighbors. \n",
    "# As the number of dimensions (features) increases, the distance between data points in the high-dimensional space can become less meaningful.\n",
    "# This is because in high dimensions, all vectors become almost equidistant to the search query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c322108a-bc1c-491a-b529-1264b2800d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Handling missing values in the K-Nearest Neighbors (KNN) algorithm can be done using a method known as KNN Imputation. \n",
    "# Here’s how it works:\n",
    "\n",
    "# Identify Missing Values: The first step is to identify the missing values in your dataset.\n",
    "\n",
    "# KNN Imputation: The KNN Imputation method replaces missing values using the values of K nearest neighbors. \n",
    "# It calculates distances from the instance with the missing value to every other instance in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e32c8d5-ee97-4999-b00d-944126d44ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# KNN Classifier:\n",
    "\n",
    "# The KNN classifier predicts the class of a new data point by looking at the classes of its K nearest neighbors.\n",
    "# The new data point is assigned to the class that has the highest frequency among its K nearest neighbors.\n",
    "# It’s used for categorical predictions21.\n",
    "# The performance of KNN classifier can be measured using several metrics such as accuracy, precision, recall, \n",
    "# F1 score, area under ROC curve, cross-validation, and confusion matrix.\n",
    "# KNN Regressor:\n",
    "\n",
    "# The KNN regressor predicts the value of a new data point by calculating the average of the values of its K nearest neighbors.\n",
    "# Instead of predicting a class, the regressor uses the average of all the neighbor values.\n",
    "# It’s used for continuous predictions.\n",
    "# The performance of KNN regressor can be measured using metrics such as mean squared error (MSE), root mean squared error (RMSE),\n",
    "# mean absolute error (MAE), and R-squared.\n",
    "# In terms of performance comparison, it’s not straightforward to say which one is better as it depends on the specific problem and dataset.\n",
    "# For instance, one study showed that KNN from data simulation provided better performance in accuracy, precision, recall, classification error,\n",
    "# absolute error, and RMSE than in Linear Regression. \n",
    "# However, it’s always a good idea to experiment with both and choose the one that provides the best results for your specific dataset and problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef00811e-6ed1-4660-ac85-4233f6854a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# Strengths:\n",
    "\n",
    "# Simplicity: KNN is simple to implement.\n",
    "# Flexibility: KNN can be used for both regression tasks and classification tasks.\n",
    "# No Training Required: KNN does not require any training, which saves computational resources.\n",
    "# Robust to Noisy Data: KNN is robust to noisy data, as it relies on the majority vote of nearest neighbors.\n",
    "\n",
    "\n",
    "# Weaknesses:\n",
    "\n",
    "# Choice of K: An appropriate selection of K value can be tricky.\n",
    "# Computationally Intensive: KNN can be computationally intensive, especially for large datasets.\n",
    "# Sensitive to Irrelevant Features: KNN can be sensitive to irrelevant or redundant features because all features contribute \n",
    "# equally to the calculation of distance.\n",
    "# Sensitive to the Scale of Data: KNN is sensitive to the scale of the data. \n",
    "# Features with a larger scale can dominate the distance metric, making the algorithm biased towards those features.\n",
    "\n",
    "\n",
    "# Addressing Weaknesses:\n",
    "\n",
    "# Choice of K: Cross-validation can be used to select an optimal K value.\n",
    "# Computationally Intensive: Dimensionality reduction techniques, such as Principal Component Analysis (PCA), can be used to \n",
    "# reduce the computational cost.\n",
    "# Sensitive to Irrelevant Features: Feature selection techniques can be used to select relevant features and discard irrelevant or redundant ones.\n",
    "# Sensitive to the Scale of Data: Data normalization or standardization can be used to bring all features to the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94ee0e37-ac4a-46c1-992e-06bdbea46435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9\n",
    "\n",
    "# Euclidean Distance:\n",
    "\n",
    "# Euclidean Distance represents the shortest distance between two points12.\n",
    "# It’s often the “default” distance used in KNN to find the “k closest points” of a particular sample point1.\n",
    "# It’s calculated using Pythagoras’ theorem1.\n",
    "# The square of the total distance between two objects is the sum of the squares of the distances along each perpendicular coordinate1.\n",
    "# Manhattan Distance:\n",
    "\n",
    "# Manhattan Distance is the sum of absolute differences between points across all the dimensions12.\n",
    "# It’s also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance1.\n",
    "# It’s the total sum of the difference between the x-coordinates and y-coordinates1.\n",
    "# It’s used in regression analysis to find a straight line that fits a given set of points1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bd8a9-0c5b-4d10-a109-cd4d1ed2d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10\n",
    "\n",
    "# Feature scaling plays a crucial role in the K-Nearest Neighbors (KNN) algorithm. Here’s why:\n",
    "\n",
    "# Distance Calculation: KNN is a distance-based algorithm that calculates the distance between different points to make predictions. \n",
    "# If the scale of the features is not the same, features with larger scales can dominate the distance calculations.\n",
    "\n",
    "# Preventing Bias: Feature scaling helps in preventing features with larger magnitudes from dominating the distance calculations.\n",
    "# This ensures that the algorithm is not biased towards variables with higher magnitude.\n",
    "\n",
    "# Improving Performance: Feature scaling can improve the performance of the KNN algorithm.\n",
    "# It brings all the variables to the same scale, making it easier for the algorithm to calculate distances.\n",
    "\n",
    "# Standardization: Feature scaling through standardization, also called Z-score normalization, is an important preprocessing step for\n",
    "# many machine learning algorithms2. It involves rescaling each feature such that it has a standard deviation of 1 and a mean of 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
