{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db4bdc3-9629-4976-90ec-90e5658ba48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# Min-Max scaling is a normalization technique that enables us to scale data in a dataset to a specific range using each feature’s minimum and maximum value. \n",
    "# It is used in data preprocessing to scale the data between 0 and 1. This normalization helps us to understand the data easily. The transformation is given by:\n",
    "\n",
    "# X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0)) X_scaled = X_std * (max - min) + min\n",
    "\n",
    "# where min, max = feature_range. This transformation is often used as an alternative to zero mean, unit variance scaling.\n",
    "\n",
    "# For example, suppose we have a dataset with values ranging from 0 to 100. We can use Min-Max scaling to scale the data between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "# x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "# where x is the original value, x_min is the minimum value in the dataset, and x_max is the maximum value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21b2a0f-3af2-41e8-aadb-b92ecb59549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# The Unit Vector technique in feature scaling is a normalization technique that scales the data so that each feature vector has a length of 1. \n",
    "# This is done by dividing each feature vector by its magnitude. This technique is useful when dealing with features with hard boundaries. \n",
    "# For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "\n",
    "# The difference between Min-Max scaling and Unit Vector scaling is that Min-Max scaling scales the data between 0 and 1 using each feature’s minimum and maximum value. \n",
    "# On the other hand, Unit Vector scaling scales the data so that each feature vector has a length of 1.\n",
    "\n",
    "# For example, suppose we have a dataset with values ranging from 0 to 100. We can use Min-Max scaling to scale the data between 0 and 1. \n",
    "# The formula for Min-Max scaling is:\n",
    "\n",
    "# x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "# where x is the original value, x_min is the minimum value in the dataset, and x_max is the maximum value in the dataset.\n",
    "\n",
    "# On the other hand, suppose we have a dataset with two features: age and income. We can use Unit Vector scaling to scale the data so that each feature vector has a length of 1. \n",
    "# The formula for Unit Vector scaling is:\n",
    "\n",
    "# x_scaled = x / ||x||\n",
    "\n",
    "# where x is the original feature vector and ||x|| is its magnitude1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5747027-ed7b-4cef-90d1-2900c836d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# Principal Component Analysis (PCA) is a statistical technique that is used for dimensionality reduction. \n",
    "# It is an unsupervised learning algorithm that identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data. \n",
    "# The principal components are linear combinations of the original variables in the dataset and are ordered in decreasing order of importance.\n",
    "\n",
    "# PCA is used to reduce the dimensionality of a dataset while retaining as much of the original information as possible. \n",
    "# It is useful when dealing with high-dimensional data where the amount of data required to obtain a statistically significant result increases exponentially. \n",
    "# This can lead to issues such as overfitting, increased computation time, and reduced accuracy of machine learning models.\n",
    "\n",
    "# For example, suppose we have a dataset with 10 features. We can use PCA to reduce the number of features while retaining as much of the original information as possible. \n",
    "# The result might be a new dataset with only 3 features that capture most of the variance in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5498cd-47b6-441a-9b6f-4bf085f77af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# PCA can be used for feature extraction. Feature extraction is a technique that involves transforming the original features of a dataset into a new set of features that are more informative and easier to work with.\n",
    "# PCA is one of the most popular feature extraction techniques that is used to reduce the dimensionality of a dataset while retaining as much of the original information as possible.\n",
    "\n",
    "# PCA works by identifying a set of orthogonal axes, called principal components, that capture the maximum variance in the data. \n",
    "# These principal components are linear combinations of the original variables in the dataset and are ordered in decreasing order of importance. \n",
    "# By selecting only the top k principal components, we can reduce the dimensionality of the dataset while retaining most of the original information.\n",
    "\n",
    "# For example, suppose we have a dataset with 10 features. We can use PCA to extract the top 3 principal components from the dataset.\n",
    "# These principal components will be linear combinations of the original features and will capture most of the variance in the dataset. \n",
    "# We can then use these principal components as new features in our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e33af1-80c6-4d25-98a2-186b84741c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ans 5\n",
    "\n",
    "# To preprocess the data for a recommendation system for a food delivery service, we can use Min-Max scaling to scale the data between 0 and 1. \n",
    "# This will ensure that all the features are on the same scale and will prevent features with larger values from dominating the model.\n",
    "\n",
    "# For example, suppose we have a dataset with features such as price, rating, and delivery time. \n",
    "# We can use Min-Max scaling to scale each feature between 0 and 1. The formula for Min-Max scaling is:\n",
    "\n",
    "# x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "# where x is the original value, x_min is the minimum value in the dataset, and x_max is the maximum value in the dataset.\n",
    "\n",
    "# After scaling each feature between 0 and 1, we can then use these features to build our recommendation system.\n",
    "# We can use techniques such as collaborative filtering or content-based filtering to recommend food items to users based on their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd1561c3-c3fc-46cd-ba77-1a4952cd4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# To reduce the dimensionality of the dataset for a model to predict stock prices, we can use PCA. \n",
    "# PCA is a technique that identifies a set of orthogonal axes, called principal components, that capture the maximum variance in the data. \n",
    "# These principal components are linear combinations of the original variables in the dataset and are ordered in decreasing order of importance.\n",
    "\n",
    "# To use PCA to reduce the dimensionality of the dataset, we would first standardize the data by subtracting the mean and dividing by the standard deviation. \n",
    "# We would then compute the covariance matrix of the standardized data. We would then compute the eigenvectors and eigenvalues of the covariance matrix. \n",
    "# The eigenvectors represent the principal components, and the eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "# We would then select only the top k principal components that capture most of the variance in the data. \n",
    "# By selecting only these top k principal components, we can reduce the dimensionality of the dataset while retaining most of the original information.\n",
    "\n",
    "# For example, suppose we have a dataset with many features such as company financial data and market trends. \n",
    "# We can use PCA to extract only the top k principal components from the dataset. \n",
    "# These principal components will be linear combinations of the original features and will capture most of the variance in the dataset. \n",
    "# We can then use these principal components as new features in our machine learning model to predict stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001683fe-5b71-402d-baa1-67e461d109e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Ans 7\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data=[1,5,10,15,20]\n",
    "scaler=MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_data=scaler.fit_transform([[x] for x in data])\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7688822-7972-4bdb-8aa9-c370624b360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [9.65314794e-01 3.46852064e-02 6.50151632e-34]\n",
      "Cumulative Explained Variance: [0.96531479 1.         1.        ]\n",
      "Number of Principal Components to Retain: 1\n"
     ]
    }
   ],
   "source": [
    "# Ans 8\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data \n",
    "data = np.array([\n",
    "    [170, 65, 30, 1, 120],\n",
    "    [160, 55, 25, 0, 130],\n",
    "    [175, 70, 35, 1, 140],\n",
    "    # ... more data points ...\n",
    "])\n",
    "\n",
    "# Separate features (X) and target (y) if applicable\n",
    "X = data[:, :-1]  # Exclude the last column (blood pressure)\n",
    "y = data[:, -1]   # Target variable (blood pressure)\n",
    "\n",
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "\n",
    "num_components_to_retain = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
    "\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance:\", cumulative_explained_variance)\n",
    "print(\"Number of Principal Components to Retain:\", num_components_to_retain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc142307-2d87-47e3-8576-19459246b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A common rule of thumb is to retain enough principal components to explain at least 80% of the variance in the data.\n",
    "# Hence we will choose 80% of 5 i.e 4 principal components as principal features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
