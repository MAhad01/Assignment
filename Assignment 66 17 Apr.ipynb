{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93cf2f16-d309-4683-930e-f9318cf15338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# Gradient Boosting Regression is a powerful machine learning technique used for regression tasks.\n",
    "# It produces a prediction model in the form of an ensemble of weak prediction models.\n",
    "\n",
    "# Here’s how it works:\n",
    "\n",
    "# Sequential Learning: The algorithm builds a model in a stage-wise fashion and each new model tries to correct the previous model.\n",
    "# It combines several weak learners into strong learners.\n",
    "# Minimize Loss Function: Each new model is trained to minimize the loss function such as mean squared error or cross-entropy of the previous\n",
    "# model using gradient descent. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of \n",
    "# the current ensemble and then trains a new weak model to minimize this gradient.\n",
    "# Residual Errors: In contrast to AdaBoost, the weights of the training instances are not tweaked, instead, each predictor is trained using\n",
    "# the residual errors of the predecessor as labels.\n",
    "# Gradient Boosted Trees: There is a technique called the Gradient Boosted Trees whose base learner is CART (Classification and Regression Trees).\n",
    "\n",
    "# Shrinkage: There is an important parameter used in this technique known as Shrinkage. Shrinkage refers to the fact that the prediction of each \n",
    "# tree in the ensemble is shrunk after it is multiplied by the learning rate (eta) which ranges between 0 to 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4a7d0-0859-4a8b-9352-45010b9d5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the base learner\n",
    "def base_learner(X, y, weights):\n",
    "    return np.polyfit(X, y, 1, w=weights)\n",
    "\n",
    "# Define the boosting algorithm\n",
    "def gradient_boosting(X, y, M):\n",
    "    N = len(y)\n",
    "    weights = np.ones(N) / N\n",
    "    models = []\n",
    "    for m in range(M):\n",
    "        learner = base_learner(X, y, weights)\n",
    "        models.append(learner)\n",
    "        predictions = np.polyval(learner, X)\n",
    "        residuals = y - predictions\n",
    "        weights = np.abs(residuals)\n",
    "        weights /= np.sum(weights)\n",
    "    return models\n",
    "\n",
    "# Define the prediction function\n",
    "def predict(models, X):\n",
    "    return sum(np.polyval(model, X) for model in models)\n",
    "\n",
    "# Define the mean squared error function\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Define the R-squared function\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.var(y_true) * len(y_true)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "# Generate a small dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100)\n",
    "y = X ** 2 + np.random.randn(100) * 0.1\n",
    "\n",
    "# Train the model\n",
    "models = gradient_boosting(X, y, 10)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = predict(models, X)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r_squared(y, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cece4d-064e-4e2a-8fa2-5825ab5854b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a small dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'n_estimators': [10, 100, 1000],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Define the grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25cb97c0-db2d-4349-ae74-6fc797cd7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANs 4\n",
    "\n",
    "# In the context of Gradient Boosting, a weak learner refers to a simple model that performs only slightly better than random chance. \n",
    "# The term \"weak\" does not imply that these learners are bad, but rather that they are not complex. \n",
    "\n",
    "# In Gradient Boosting, decision trees are commonly used as weak learners. Specifically, regression trees are used that output real values \n",
    "# for splits and whose output can be added together, allowing subsequent models' outputs to be added and \"correct\" the residuals in the predictions.\n",
    "\n",
    "# The algorithm starts with a single weak learner and works as follows:\n",
    "# 1. Train a single weak learner.\n",
    "# 2. Figure out which examples the weak learner got wrong.\n",
    "# 3. Build another weak learner that focuses on the areas the first weak learner got wrong.\n",
    "# 4. Continue this process until a predetermined stopping condition is met, such as until a set number of weak learners have been created, \n",
    "# or the model’s performance has plateaued.\n",
    "\n",
    "# In this way, each new weak learner is specifically tuned to focus on the weak points of the previous weak learner(s).\n",
    "# The more often an example is missed, the more likely it is that the next weak learner will be the one that can classify that example correctly.\n",
    "# In this way, all the weak learners work together to make up a single strong learner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2699e916-75a8-439e-bb4b-68314a8fa61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANs 5\n",
    "\n",
    "# The intuition behind the Gradient Boosting algorithm is that it repetitively leverages the patterns in residuals and strengthens a model\n",
    "# with weak predictions to make it better.\n",
    "# Here’s how it works:\n",
    "\n",
    "# Initial Model: The algorithm starts by fitting an initial model (e.g., a tree or linear regression) to the data.\n",
    "\n",
    "# Sequential Learning: Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly.\n",
    "# The combination of these two models is expected to be better than either model alone.\n",
    "# Minimize Error: Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models.\n",
    "# The main intuition behind the algorithm is that the best possible next model, when combined with previous models, minimizes the overall \n",
    "# prediction error.\n",
    "# Target Outcomes: The key idea is to set the target outcomes for this next model to minimize the error.\n",
    "# Residuals: So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model\n",
    "# with weak predictions and make it better. Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop \n",
    "# modeling residuals (otherwise it might lead to overfitting).\n",
    "\n",
    "# Negative Gradient: The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated \n",
    "# with the negative gradient of the loss function, associated with the whole ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38eb8c8-89cd-4edd-a696-2ad300d09194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Gradient Boosting builds an ensemble of weak learners in a sequential manner. Here’s how it works:\n",
    "\n",
    "# Initial Model: The algorithm starts by fitting an initial model (e.g., a tree or linear regression) to the data.\n",
    "\n",
    "# Sequential Learning: Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly.\n",
    "# The combination of these two models is expected to be better than either model alone1.\n",
    "\n",
    "# Minimize Error: Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models.\n",
    "# The main intuition behind the algorithm is that the best possible next model, when combined with previous models, minimizes the overall prediction error1.\n",
    "\n",
    "# Target Outcomes: The key idea is to set the target outcomes for this next model to minimize the error.\n",
    "\n",
    "# Residuals: So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model \n",
    "# with weak predictions and make it better3. Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop \n",
    "# modeling residuals (otherwise it might lead to overfitting).\n",
    "\n",
    "# Negative Gradient: The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated with the negative \n",
    "# gradient of the loss function, associated with the whole ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45854961-84e3-447a-b188-7250aa9ddaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# Initial Model: The algorithm starts by fitting an initial model (e.g., a tree or linear regression) to the data1234. This model will be associated with a residual (y – F0)3.\n",
    "\n",
    "# Sequential Learning: Then a second model is built that focuses on accurately predicting the cases where the first model performs poorly1234. The combination of these two models is expected to be better than either model alone1234.\n",
    "\n",
    "# Minimize Error: Each successive model attempts to correct for the shortcomings of the combined boosted ensemble of all previous models1234. The main intuition behind the algorithm is that the best possible next model, when combined with previous models, minimizes the overall prediction error1234.\n",
    "\n",
    "# Target Outcomes: The key idea is to set the target outcomes for this next model to minimize the error1234.\n",
    "\n",
    "# Residuals: So, the intuition behind gradient boosting algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better1234. Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting)1234.\n",
    "\n",
    "# Negative Gradient: The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated with the negative gradient of the loss function, associated with the whole ensemble1234.\n",
    "\n",
    "# Combine Models: Now, F0 and h1 are combined to give F1, the boosted version of F03. The mean squared error from F1 will be lower than that from F03."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
