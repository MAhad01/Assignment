{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3392b3f-875b-4be2-bb00-46c5883f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# GridSearchCV is a technique used in machine learning to search and find the optimal combination of hyperparameters for a given model. \n",
    "# It systematically explores a predefined set of hyperparameter values, creating a “grid” of possible combinations. \n",
    "# The GridSearchCV class in Scikit-learn serves a dual purpose in tuning your mode. \n",
    "# The class allows you to apply a grid search to an array of hyper-parameters, and cross-validate your model using k-fold cross validation.\n",
    "# This function helps to loop through predefined hyperparameters and fit your estimator (model) on your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e803797d-8291-46c5-97ef-82fcf9292475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# GridSearchCV and RandomizedSearchCV are two techniques used in machine learning to search and find the optimal combination of hyperparameters for a given model. \n",
    "# The main difference between the two is that GridSearchCV systematically explores a predefined set of hyperparameter values, creating a “grid” of possible combinations, \n",
    "# while RandomizedSearchCV randomly samples from the hyperparameter space.\n",
    "\n",
    "# GridSearchCV is exhaustive and can be computationally expensive if the search space is large (e.g. very many hyperparameters). \n",
    "# On the other hand, RandomizedSearchCV can be useful when there are many hyperparameters, so the search space is large. \n",
    "# It can be used if you have a prior belief on what the hyperparameters should be.\n",
    "\n",
    "# In general, GridSearchCV is slower than RandomizedSearchCV.\n",
    "# When there are less rows of data then we go for GridSearchCV but when there is very Big data set then we go for RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029cd2a9-e35e-4ad7-9d45-b8eb00b91310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# Data leakage is a problem in machine learning when developing predictive models. \n",
    "# It occurs when information from outside the training dataset is used to create the model. \n",
    "# This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the \n",
    "# estimated performance of the model being constructed.\n",
    "\n",
    "# Data leakage can cause you to create overly optimistic if not completely invalid predictive models. \n",
    "# It is a serious problem for at least 3 reasons:\n",
    "\n",
    "# It is a problem if you are running a machine learning competition. \n",
    "# Top models will use the leaky data rather than be good general model of the underlying problem.\n",
    "# It is a problem when you are a company providing your data. \n",
    "# Reversing an anonymization and obfuscation can result in a privacy breach that you did not expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9c612b-1284-4b98-8351-c4fd0aff383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# There are several ways to prevent data leakage when building a machine learning model. Here are some tips to help you prevent data leakage:\n",
    "\n",
    "# Temporal Cutoff: Remove all data just prior to the event of interest, focusing on the time you learned about a fact or event.\n",
    "# Add Noise: Add random noise to input data to try and smooth out the effects of possibly leaking variables.\n",
    "# Remove Leaky Variables: Evaluate simple models that use only one variable at a time and remove those that are too good.\n",
    "# Ensure your data is secure and encrypted: Implement a strong data governance policy.\n",
    "# Set up user authentication protocols: Audit and monitor data access.\n",
    "# Train employees to recognize potential data leakage risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e0bef1-4a16-46a6-989a-3712f7325537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data. \n",
    "# It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. \n",
    "# The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.\n",
    "\n",
    "# A good model is one which has high TP and TN rates, while low FP and FN rates. \n",
    "# This matrix aids in analyzing model performance, identifying mis-classifications, and improving predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd535669-d8c4-4387-a01a-8e10fb648c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Precision and recall are two important metrics used to evaluate the performance of a classification model.\n",
    "# They are both derived from the confusion matrix, which is a table that summarizes the number of true positives (TP), true negatives (TN), false positives (FP), \n",
    "# and false negatives (FN) produced by the model on a set of test data.\n",
    "\n",
    "# Precision, also known as positive predictive value, is the ratio of correct positive predictions to the total predicted positives.\n",
    "# It is calculated as TP / (TP + FP). Precision measures how many of the positive predictions made by the model are actually positive.\n",
    "\n",
    "# Recall, also known as sensitivity or true positive rate, is the ratio of correct positive predictions to the total number of actual positives. \n",
    "# It is calculated as TP / (TP + FN). Recall measures how many of the actual positives were correctly identified by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3bc0db0-e577-4c78-8eba-d685badaef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. \n",
    "# It displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data. \n",
    "# The matrix can be used to determine which types of errors the model is making by analyzing the values of FP and FN.\n",
    "\n",
    "# False positives (FP) are “false alarms” where the model incorrectly predicts a positive outcome when the actual outcome is negative. \n",
    "# False negatives (FN) are missed cases where the model incorrectly predicts a negative outcome when the actual outcome is positive. \n",
    "# By analyzing these values, you can determine if your model is making more false positive or false negative errors, and take steps to improve its performance accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7adff33f-de5b-456d-9cd7-9e0be7f66b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. \n",
    "# It displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data. \n",
    "# From these values, several common metrics can be derived to evaluate the performance of the model.\n",
    "\n",
    "# Some common metrics that can be derived from a confusion matrix include:\n",
    "\n",
    "# Accuracy: The ratio of correct predictions to the total number of predictions. It is calculated as (TP + TN) / (TP + TN + FP + FN).\n",
    "# Precision: The ratio of correct positive predictions to the total predicted positives. It is calculated as TP / (TP + FP).\n",
    "# Recall: The ratio of correct positive predictions to the total number of actual positives. It is calculated as TP / (TP + FN).\n",
    "# F1 Score: The harmonic mean of precision and recall. It is calculated as 2 * (Precision * Recall) / (Precision + Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c729692-6078-4452-a835-69169c747c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9\n",
    "\n",
    "# The accuracy of a classification model is the ratio of correct predictions to the total number of predictions. \n",
    "# It is calculated as (TP + TN) / (TP + TN + FP + FN), where TP, TN, FP, and FN are the values in the confusion matrix. \n",
    "# The confusion matrix is a table that summarizes the performance of a classification model on a set of test data. \n",
    "# It displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.\n",
    "\n",
    "# In summary, the accuracy of a classification model is directly related to the values in its confusion matrix. \n",
    "# A good model is one which has high TP and TN rates, while low FP and FN rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e362f86-005e-4065-a14c-9ceab8e624a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10\n",
    "\n",
    "# A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. \n",
    "# It displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data. \n",
    "# The matrix can be used to identify potential biases or limitations in your machine learning model by analyzing the values of TP, TN, FP, and FN.\n",
    "\n",
    "# One way to identify potential biases or limitations is to look for imbalanced class distributions in the confusion matrix. \n",
    "# If one class has significantly more instances than another, it can cause the model to be biased towards predicting the majority class, \n",
    "# resulting in a high accuracy but poor performance on the minority class.\n",
    "\n",
    "# Another way to identify potential biases or limitations is to look for patterns in the types of errors being made by the model. \n",
    "# For example, if the model is consistently misclassifying one class as another, it may indicate that there is a problem with the feature representation or \n",
    "# that additional features are needed to better distinguish between the classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
