{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "370c2000-3ea2-425e-a5c7-4779bead231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# An ensemble technique in machine learning is a method that combines the predictions of multiple models to produce a more accurate or robust output. \n",
    "# Ensemble techniques can improve the performance of machine learning systems by reducing the errors or biases that may exist in individual models.\n",
    "# There are different types of ensemble techniques, such as bagging, boosting, stacking, and voting. \n",
    "# Each type has its own advantages and disadvantages, depending on the problem and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520408b6-9546-4b7f-a7d4-f24dc7cf8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "# Improve Accuracy: Ensemble methods usually produce more accurate solutions than a single model would. \n",
    "# They combine the decisions from multiple models to improve the overall performance.\n",
    "\n",
    "# Mitigate Errors: Ensemble learning aims to mitigate errors or biases that may exist in individual models by leveraging the collective \n",
    "# intelligence of the ensemble.\n",
    "\n",
    "# Resilience Against Uncertainties: By considering multiple perspectives and utilizing the strengths of different models, \n",
    "# ensemble learning provides resilience against uncertainties in the data.\n",
    "\n",
    "# Diverse Model Combination: Ensemble learning helps improve machine learning results by combining several models. \n",
    "# This approach allows the production of better predictive performance compared to a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64026de0-bcce-4fb5-9425-d2a6c89537e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# Here’s how Baging works:\n",
    "\n",
    "# Random Sampling: Bagging involves taking random subsets of the original dataset, with replacement. \n",
    "# This means that individual data points can be chosen more than once.\n",
    "\n",
    "# Model Training: Each of these subsets is then used to train a separate model.\n",
    "# These models are often referred to as base models.\n",
    "\n",
    "# Aggregation: The predictions from all the individual models are then aggregated to give the final prediction.\n",
    "# This can be done through methods like voting for classification problems or averaging for regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942e5817-8805-4994-b010-5bf2a81621be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# Boosting is an ensemble learning method in Machine Learning. Here’s how it works:\n",
    "\n",
    "# Sequential Training: Boosting involves training multiple models sequentially. The first model is built from the training data.\n",
    "# Then the second model is built which tries to correct the errors present in the first model.\n",
    "# This procedure is continued and models are added until either the complete training data set is predicted correctly or \n",
    "# the maximum number of models are added.\n",
    "\n",
    "# Weight Adjustment: After each model is trained, the weights of the misclassified instances are increased.\n",
    "# This means that subsequent models focus more on the instances that previous models classified incorrectly.\n",
    "\n",
    "# Model Combination: The predictions from all the individual models are then combined to give the final prediction.\n",
    "# This can be done through methods like voting for classification problems or averaging for regression problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127cd5df-1b0f-4570-9dc0-09b25d91ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# Ensemble techniques in machine learning offer several benefits:\n",
    "\n",
    "# Improved Accuracy: By combining the predictions of multiple models, ensemble methods can often achieve higher accuracy than any of the individual models could on their own.\n",
    "\n",
    "# Robustness: Ensemble methods are more robust to noise and outliers. They average out biases and reduce variance, leading to more stable and robust predictions.\n",
    "\n",
    "# Better Generalization: As ensemble methods aggregate the predictions of multiple models, they are less likely to overfit to the training data and hence, they generalize better to unseen data.\n",
    "\n",
    "# Handling High Dimensionality: Ensemble methods can effectively handle high dimensional spaces as well as large amounts of training data.\n",
    "\n",
    "# Error Reduction: They can achieve lower error rates as long as the base learners perform better than random guessing.\n",
    "\n",
    "# Model Diversity: Ensemble learning encourages diversity among the models. Each model captures different aspects of the data, which leads to a more holistic understanding of the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfad1a28-32f9-471e-8905-67f7345f4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Ensemble techniques are not always better than individual models. Here are a few reasons why:\n",
    "\n",
    "# Data Compatibility: The model that is closest to the true data generating process will always be best and will beat most ensemble methods.\n",
    "# So if the data come from a linear process, linear models will be much superior to ensemble models.\n",
    "\n",
    "# Interpretability: Ensemble models often suffer from a lack of interpretability.\n",
    "# While they can provide more accurate predictions, they can also be more complex and harder to understand compared to individual models.\n",
    "\n",
    "# Overfitting: Some ensemble methods, like boosting, can overfit with noisy data and outliers.\n",
    "\n",
    "# Increased Bias: Some ensemble methods like bagging can increase bias.\n",
    "\n",
    "# Time and Computation: Ensemble methods can be complex and time-consuming to implement.\n",
    "# They require training multiple models, which can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddeb604c-1927-4a2b-95e7-3b2ce4438e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# Calculating confidence intervals using the bootstrap method involves two main steps:\n",
    "\n",
    "# Calculate a Population of Statistics: The first step is to use the bootstrap procedure to resample the original data a number of times and calculate the statistic of interest.\n",
    "# The dataset is sampled with replacement, meaning that each time an item is selected from the original dataset, it is not removed,\n",
    "# allowing that item to possibly be selected again for the sample.\n",
    "# The statistic is calculated on the sample and stored so that we build up a population of the statistic of interest\n",
    "\n",
    "# Calculate Confidence Interval: Now that we have a population of the statistics of interest, we can calculate the confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7a0bf5-0b56-465d-bf6a-d7b3cf6d4c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# The bootstrap method is a statistical procedure that involves resampling a single dataset to create many simulated samples.\n",
    "# Here are the steps involved in the bootstrap method\n",
    "# Resample: The first step is to resample the original data a number of times.\n",
    "# The dataset is sampled with replacement, meaning that each time an item is selected from the original dataset, it is not removed, \n",
    "# allowing that item to possibly be selected again for the sample.\n",
    "\n",
    "# Calculate Statistics: For each of these resampled datasets, calculate the statistic of interest.\n",
    "# This could be any summary statistic such as the mean or standard deviation.\n",
    "\n",
    "# Repeat: Repeat these steps a large number of times, typically thousands or tens of thousands of times, \n",
    "# to build up a distribution of your statistic.\n",
    "\n",
    "# Estimate Confidence Interval: Once you have a population of your statistic from the resampled datasets, you can calculate confidence intervals.\n",
    "# This can be done by taking the middle 95% of bootstrap statistics for a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767fca96-99a3-49ef-8b1c-44e0a979063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_heights = np.random.normal(15, 2, 50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstrap = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_means = np.array([np.random.choice(sample_heights, len(sample_heights)).mean() for _ in range(n_bootstrap)])\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(f\"The 95% confidence interval for the population mean height is {confidence_interval}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
