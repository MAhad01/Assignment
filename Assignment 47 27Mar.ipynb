{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b37bf84f-7bea-4cd5-909b-94973ce8c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# R-squared is a goodness-of-fit measure for linear regression models. \n",
    "# It measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale. \n",
    "# It indicates the percentage of the variance in the dependent variable that the independent variables explain collectively.\n",
    "\n",
    "# R-squared is always between 0 and 100%: 0% represents a model that does not explain any of the variation in the response variable around its mean. \n",
    "# The mean of the dependent variable predicts the dependent variable as well as the regression model. \n",
    "# 100% represents a model that explains all the variation in the response variable around its mean.\n",
    "\n",
    "# The formula for R-squared is:\n",
    "# R2=Explained variation/Total variation = 1−Unexplained variation/Total variation\n",
    "# where:\n",
    "\n",
    "# Explained variation is the variation in the response variable that is explained by the independent variables.\n",
    "# Unexplained variation is the variation in the response variable that cannot be explained by the independent variables.\n",
    "# Total variation is the total variance in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c68938-b75a-4e6b-8289-61a63c7d11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model. \n",
    "\n",
    "# It is calculated as:\n",
    "# Adjusted R2=1−n−k−1(1−R2)(n−1)\n",
    "# where:\n",
    "\n",
    "# R-squared is the coefficient of determination.\n",
    "# n is the sample size.\n",
    "# k is the number of independent variables.\n",
    "\n",
    "# The adjusted R-squared value is always lower than the R-squared value. \n",
    "# The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. \n",
    "# It decreases when a predictor improves the model by less than expected by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19ccca49-d3bc-4021-9895-027a1eb77884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANs 3\n",
    "\n",
    "# Adjusted R-squared is more appropriate when comparing models that have different numbers of independent variables or when overfitting is a concern.\n",
    "# It is a modified version of R-squared that takes into account the number of independent variables in the model. \n",
    "# It penalizes models that have too many variables, making it more accurate in cases where overfitting is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "316c7ce0-604b-4c8e-a593-19e7f7aae82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and \n",
    "# MAE (Mean Absolute Error) are metrics used to evaluate the performance of a regression model.\n",
    "\n",
    "# RMSE is the square root of the average of squared differences between predicted and actual values. \n",
    "# It is a measure of the difference between the observed and predicted values of a dependent variable.\n",
    "\n",
    "# MSE is the average of the squared difference between the actual value and predicted values by the regression model. \n",
    "# The reason for squaring the error is to remove any negative signs. By squaring the error, MSE penalizes the error more than MAE.\n",
    "\n",
    "# MAE is the average of absolute differences between predicted and actual values. It is less sensitive to outliers than MSE.\n",
    "# The lower value of RMSE, MSE, and MAE implies higher accuracy of a regression model. However, a higher value of R-squared is considered desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ebc100-8ad2-43b5-bb85-3751e94bf253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# The advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis are:\n",
    "\n",
    "# RMSE is more sensitive to outliers than MAE. It is also harder to interpret than MAE.\n",
    "# MSE is not expressed on the same scale as the dependent variable, making this metric somewhat difficult to interpret.\n",
    "# MAE is less sensitive to outliers than MSE. It is expressed on the same scale as the dependent variable, making it easier to interpret.\n",
    "# The choice of metric depends on the specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bb094e-0500-43cc-b374-ab81aa7355c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# Lasso regularization is a technique used in regression analysis to prevent overfitting by adding a penalty term to the cost function.\n",
    "# The penalty term is the absolute value of the magnitude of the coefficients multiplied by a constant alpha. \n",
    "# The Lasso method can be used for feature selection because it shrinks the coefficients of less important features to zero.\n",
    "\n",
    "# Ridge regularization is another technique used in regression analysis to prevent overfitting by adding a penalty term to the cost function. \n",
    "# The penalty term is the square of the magnitude of the coefficients multiplied by a constant alpha. \n",
    "# Ridge regression shrinks the coefficients of less important features towards zero but does not set them exactly to zero.\n",
    "\n",
    "# Lasso regularization is more appropriate when there are many features with high correlation and you need to take away the useless features. \n",
    "# Ridge regularization is more appropriate when there are many features with multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe384b24-cf23-4ff7-b828-c1280e56db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7\n",
    "\n",
    "# Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the cost function. \n",
    "# The penalty term is a function of the magnitude of the coefficients.\n",
    "# The goal is to shrink the coefficients towards zero, which reduces the complexity of the model and prevents overfitting12.\n",
    "\n",
    "# For example, let’s say you have a dataset with 1000 features and only 100 observations. \n",
    "# You want to build a linear regression model to predict the target variable. \n",
    "# If you use all 1000 features, you may end up with an overfitted model that performs well on the training data but poorly on the test data.\n",
    "# Regularization techniques such as Lasso and Ridge can help you avoid overfitting by shrinking the coefficients of less important features towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97566b00-6103-45d8-a47d-284aa2efa69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8\n",
    "\n",
    "# The limitations of regularized linear models are:\n",
    "\n",
    "# Regularization may not always improve the performance of a model. It depends on the specific problem.\n",
    "# Regularization may not work well when there are many important features.\n",
    "# Regularization may not work well when there is a high degree of multicollinearity among the features.\n",
    "# Regularized linear models may not always be the best choice for regression analysis because they assume that the relationship \n",
    "# between the independent variables and the dependent variable is linear.\n",
    "# If the relationship is nonlinear, then other regression techniques such as decision trees or neural networks may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5fba10f-9e8c-4457-96bd-b137ef9d9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9\n",
    "\n",
    "# The choice of evaluation metric depends on the specific problem. \n",
    "# However, in general, a lower value of RMSE or MAE implies higher accuracy of a regression model.\n",
    "# In this case, Model B has a lower MAE than Model A, which implies that Model B is more accurate than Model A.\n",
    "\n",
    "# The limitations of using RMSE or MAE as evaluation metrics are:\n",
    "\n",
    "# They do not provide information about the direction of the error (overestimation or underestimation).\n",
    "# They do not provide information about the relative importance of different errors.\n",
    "# They do not provide information about the distribution of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63622036-7a5a-4c48-a3df-8d613fefa2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10\n",
    "\n",
    "# The choice of regularization method depends on the specific problem.\n",
    "# However, in general, Ridge regularization is more appropriate when there are many features with multicollinearity, while Lasso regularization\n",
    "# is more appropriate when there are many features with high correlation and you need to take away the useless features.\n",
    "\n",
    "# The trade-off between Ridge and Lasso regularization is that Ridge regression shrinks the coefficients of less important features towards zero but does not set them exactly to zero, \n",
    "# while Lasso regression shrinks the coefficients of less important features to zero. \n",
    "# The choice of regularization method depends on the specific problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
