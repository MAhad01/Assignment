{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f8ed9c-99d7-4436-93cb-ab3a9d7e450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1\n",
    "\n",
    "# The Euclidean and Manhattan distances are two different ways of measuring distance between two points, and they are used in the K-Nearest Neighbors (KNN) algorithm.\n",
    "\n",
    "# **Euclidean Distance**:\n",
    "# - Euclidean Distance represents the shortest distance between two points.\n",
    "# - It's often the \"default\" distance used in KNN to find the \"k closest points\" of a particular sample point.\n",
    "# - It's calculated using Pythagoras' theorem.\n",
    "# - The square of the total distance between two objects is the sum of the squares of the distances along each perpendicular coordinate.\n",
    "\n",
    "# **Manhattan Distance**:\n",
    "# - Manhattan Distance is the sum of absolute differences between points across all the dimensions.\n",
    "# - It's also known as Manhattan length, rectilinear distance, L1 distance or L1 norm, city block distance, Minkowski’s L1 distance, taxi-cab metric, or city block distance.\n",
    "# - It's the total sum of the difference between the x-coordinates and y-coordinates.\n",
    "# - It's used in regression analysis to find a straight line that fits a given set of points.\n",
    "\n",
    "# The choice between Euclidean and Manhattan distance depends on the problem at hand. For example, if you were to use a Chess dataset, the use of Manhattan distance is more appropriate than Euclidean distance. Also, Manhattan Distance is preferred over the Euclidean distance metric as the dimension of the data increases due to something known as the 'curse of dimensionality'.\n",
    "\n",
    "# The difference between these two distance metrics can affect the performance of a KNN classifier or regressor. For instance, if the data points are distributed in a grid-like structure, Manhattan distance might be more appropriate. On the other hand, if the data points are distributed in a more scattered manner, Euclidean distance might be a better choice.\n",
    "# Therefore, it's always a good idea to experiment with different distance metrics and choose the one that provides the best results for your specific dataset and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3277f1-1f6a-4b56-ba8b-ad5e9162b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2\n",
    "\n",
    "# Choosing the optimal value of K in the K-Nearest Neighbors (KNN) algorithm is crucial and depends on the specific problem and dataset¹²³⁴. Here are some common conventions to keep in mind:\n",
    "\n",
    "# 1. **Odd Value**: It's recommended to choose an odd value for K to avoid ties in classification³.\n",
    "# 2. **Avoid Noise and Outliers**: If the input data has more outliers or noise, a higher value of K would be better³.\n",
    "# 3. **Rule of Thumb**: A common rule of thumb is to set K = sqrt(N)/2, where N stands for the number of samples in your training dataset².\n",
    "# 4. **Experimentation**: There is no straightforward method to calculate K. You have to experiment with different values to choose the optimal K for your problem and data¹⁴.\n",
    "\n",
    "# Remember, choosing a very low value will most likely lead to inaccurate predictions³. It's always a good idea to experiment with different values and choose the one that provides the best results for your specific dataset and problem¹²³⁴.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b14b9961-9ea5-4bfd-8256-5f1a87afa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3\n",
    "\n",
    "# The choice of distance metric significantly affects the performance of a KNN classifier or regressor¹²³⁴. The core of the KNN algorithm depends mainly on measuring the distance or similarity between the tested examples and the training examples¹. Therefore, the choice of distance measure is an important problem, as the performance of the KNN classifier or regressor is dependent on the distance/similarity measure used¹³.\n",
    "\n",
    "# Different distance metrics may yield different results, and the choice between them depends on the specific problem and dataset²³⁴. For instance, the Euclidean distance is often the default distance used in KNN and is a measure of the true straight line distance between two points². It's most appropriate when all input features have the same units (for example, all are measured in meters) or when there is only one input variable².\n",
    "\n",
    "# On the other hand, the Manhattan distance is the sum of absolute differences between points across all dimensions². It's preferred over Euclidean distance when dealing with high-dimensional data². This is because in high dimensions, all vectors become almost equidistant to the search query vector².\n",
    "\n",
    "# Moreover, experimental results show that the performance of KNN classifier depends significantly on the distance used, and there can be large gaps between the performances of different distances¹⁴. For example, a study found that a recently proposed non-convex distance performed the best when applied on most datasets comparing to the other tested distances¹⁴.\n",
    "\n",
    "# Therefore, it's always a good idea to experiment with different distance metrics and choose the one that provides the best results for your specific dataset and problem¹²³⁴.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ac5d28-9303-42b3-9eb0-169fe6a92e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4\n",
    "\n",
    "# The K-Nearest Neighbors (KNN) algorithm has several hyperparameters that can be tuned to improve the performance of the model.\n",
    "# Here are some common ones:\n",
    "\n",
    "# Number of Neighbors (K): This is the number of neighbors to consider when making predictions.\n",
    "# The choice of K can significantly impact the performance of the model.\n",
    "# A small value of K means that noise will have a higher influence on the result, and a large value makes it computationally expensive.\n",
    "\n",
    "# Distance Metric: This is the metric used to calculate the distance between data points.\n",
    "# Common choices include Euclidean distance and Manhattan distance.\n",
    "# The choice of distance metric can significantly impact the performance of the model.\n",
    "\n",
    "# Weights: This parameter allows you to specify the weight function to use in prediction.\n",
    "# Options include ‘uniform’ (all points in each neighborhood are weighted equally) and ‘distance’ (weights points by the inverse of their distance)4.\n",
    "\n",
    "# Tuning these hyperparameters can be done using techniques like grid search or random search. In grid search, you define a grid of hyperparameters and train/test your model on each of the possible combinations2. In random search, you randomly select a combination of hyperparameters to train/test your model2. \n",
    "# Cross-validation is often used in conjunction with these methods to estimate the performance of each combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15faab98-edac-4768-83ea-955d255dc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5\n",
    "\n",
    "# The size of the training set can significantly affect the performance of a KNN classifier or regressor¹⁴. Here's how:\n",
    "\n",
    "# 1. **Density of Training Data**: You should have roughly the same number of samples for each class¹. It doesn't need to be exact, but not more than 10% disparity is recommended¹. Otherwise, the boundaries will be very fuzzy¹.\n",
    "# 2. **Size of the Whole Training Set**: You need to have sufficiently enough examples in your training set so your model can generalize to unknown samples¹.\n",
    "# 3. **Noise**: KNN is very sensitive to noise by nature, so you want to avoid noise in your training set as much as possible¹.\n",
    "\n",
    "# To optimize the size of the training set, you can use techniques like cross-validation⁸. Cross-validation is a technique used to assess the performance of machine learning models⁸. It divides the data into subsets and then trains the algorithm on these subsets⁸. This allows you to make the most of your available data for training, while also ensuring that your model is able to generalize well to unseen data⁸.\n",
    "\n",
    "# Remember, the optimal size of the training set can vary for different datasets and problems¹⁴⁸. It's always a good idea to experiment with different sizes and choose the one that provides the best results for your specific dataset and problem¹⁴⁸.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1205a2dc-e0d7-43a3-ab69-3bd689aa1116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6\n",
    "\n",
    "# The K-Nearest Neighbors (KNN) algorithm has several potential drawbacks when used as a classifier or regressor¹²³⁴⁶:\n",
    "\n",
    "# 1. **Computationally Intensive**: KNN can be expensive in determining K if the dataset is large¹²³⁴⁶. It requires more memory storage than an effective classifier or supervised learning algorithms¹²³⁴⁶. In KNN, the prediction phase is slow for a larger dataset¹²³⁴⁶.\n",
    "\n",
    "# 2. **Sensitive to Noisy and Missing Data**: KNN is sensitive to outliers and missing values¹²³⁴⁶. Hence, we first need to impute the missing values and get rid of the outliers before applying the KNN algorithm¹²³⁴⁶.\n",
    "\n",
    "# 3. **Does Not Work Well with High Dimensions**: KNN does not work well with high dimensions¹²³⁴⁶. This will complicate the distance calculating process to calculate distance for each dimension¹²³⁴⁶.\n",
    "\n",
    "# 4. **Feature Scaling**: Data in all the dimensions should be scaled (normalized and standardized) properly¹²³⁴⁶.\n",
    "\n",
    "# To overcome these drawbacks and improve the performance of the model, you might consider the following strategies⁵⁶⁷⁸:\n",
    "\n",
    "# 1. **Parallelize the Computation**: One approach is to parallelize the computation (e.g., using a cluster, GPU, or multiple cores on a single machine)⁵. Rather than reducing the amount of computation, this strategy breaks the problem into multiple pieces that are solved simultaneously on different processing units⁵.\n",
    "\n",
    "# 2. **Impute Missing Values and Remove Outliers**: Before applying the KNN algorithm, you can impute the missing values and get rid of the outliers⁶.\n",
    "\n",
    "# 3. **Dimensionality Reduction**: You can implement dimension reduction methods to deal with high-dimensional data⁶. Techniques like Principal Component Analysis (PCA) or t-SNE can be used to reduce the dimensionality of the data⁶.\n",
    "\n",
    "# 4. **Feature Scaling**: You should ensure that all features are on the same scale before applying the KNN algorithm⁶. This can be achieved through normalization or standardization⁶.\n",
    "\n",
    "# 5. **Cross-Validation**: You can use cross-validation to assess the performance of the model⁸. This technique can help you determine the optimal hyperparameters for your model⁸.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
